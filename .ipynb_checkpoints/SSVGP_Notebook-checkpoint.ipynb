{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Gaussian Process Minimum Viable Code\n",
    "\n",
    "A quick notebook to show a GPflow implementation of a simple 1D streaming GP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from gpflow.ci_utils import ci_niter\n",
    "from gpflow.utilities import print_summary, set_trainable, deepcopy\n",
    "from IPython.display import clear_output\n",
    "from tensorflow_probability import distributions as tfd\n",
    "# for reproducibility of this notebook:\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_adam(model, iterations, data):\n",
    "    \"\"\"\n",
    "    Utility function running the Adam optimizer\n",
    "\n",
    "    :param model: GPflow model\n",
    "    :param interations: number of iterations\n",
    "    :param data: observed data set\n",
    "    :return logf: model training loss\n",
    "    \"\"\"\n",
    "    # Create an Adam Optimizer action\n",
    "    logf = []\n",
    "    training_loss = model.training_loss_closure(data)\n",
    "    optimizer = tf.optimizers.Adam()\n",
    "\n",
    "    @tf.function\n",
    "    def optimization_step():\n",
    "        optimizer.minimize(training_loss, model.trainable_variables)\n",
    "\n",
    "    # Optimize variational parameters \n",
    "    for step in range(iterations):\n",
    "        optimization_step()\n",
    "        if step % 10 == 0:\n",
    "            loss = -training_loss().numpy()\n",
    "            logf.append(loss)\n",
    "            clear_output(wait=True)\n",
    "            print(\"iteration {}\".format(step))\n",
    "    return logf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_continuous_opt(\n",
    "    old_model,\n",
    "    new_model,\n",
    "    data,\n",
    "    Znew,\n",
    "    Zold,\n",
    "    optimizer = None,\n",
    "    maxiter=1e3,\n",
    "    compile=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Training loop for continuous loss function Lc\n",
    "\n",
    "    :param old_model: GPflow model\n",
    "    :param new_model: GPflow model\n",
    "    :param data: observed data set\n",
    "    :param Znew: inducing points of new_model\n",
    "    :param Zold: inducing points of old_model\n",
    "    :param optimizer: tf.optimizers or tf.keras.optimizers that updates variables by applying the\n",
    "        corresponding loss gradients. Adam is a default optimizer with default settings.\n",
    "    :param maxiter: maximum number of iterations\n",
    "    :return step: iteration number\n",
    "    :return LC: model continuous training loss at step\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise optimizer and get continuous covariance matrix Kuu and mean mu_uu\n",
    "    step = []\n",
    "    Lc = []\n",
    "    KL = []\n",
    "    \n",
    "    optimizer = tf.optimizers.Adam() if optimizer is None else optimizer\n",
    "    var_list = new_model.trainable_variables\n",
    "    Kuu, mu_uu = get_Kuu(old_model, Znew, Zold)\n",
    "    \n",
    "    def optimization_step():\n",
    "        ''' Optimization step, returns model loss'''\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(var_list)\n",
    "            # Calls elbo function which returns Lc, the continuous lower-bound and KL divergences\n",
    "            f = elbo(new_model,old_model,data,Znew,Zold,Kuu,mu_uu)\n",
    "            loss = -f[0]\n",
    "            KL = f[1]\n",
    "        grads = tape.gradient(loss, var_list)\n",
    "        optimizer.apply_gradients(zip(grads, var_list))\n",
    "        return loss, KL\n",
    "    \n",
    "    if compile:\n",
    "        optimization_step = tf.function(optimization_step)\n",
    "\n",
    "    # Optimize variational parameters\n",
    "    for iter in range(int(maxiter)):\n",
    "        loss, KLstep = optimization_step()\n",
    "        if iter % 10 == 0:\n",
    "            step.append(iter)\n",
    "            Lc.append(loss)\n",
    "            KL.append(KLstep)\n",
    "            clear_output(wait=True)\n",
    "            print(\"iteration {}\".format(iter))\n",
    "            \n",
    "    return step, Lc, KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def elbo(new_model,old_model,data,Znew,Zold,Kuu,mu_uu):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to compute continuous lower-bound Lc.\n",
    "    \n",
    "    :param new_model: GPflow model\n",
    "    :param old_model: GPflow model    \n",
    "    :param data: observed data set Dnew\n",
    "    :param Znew: inducing points of new_model\n",
    "    :param Zold: inducing points of old_model\n",
    "    :param Kuu: continuous covariance matrix\n",
    "    :param mu_uu: continuous mean\n",
    "    :return LC: model continuous training loss at step\n",
    "    :return KL: KL divergences of prior and posterior distributions\n",
    "    \"\"\"\n",
    "    \n",
    "    X, Y = data\n",
    "    \n",
    "    # get mean and variance of latent function at new data points\n",
    "    f_mean, f_var = new_model.predict_f(X)\n",
    "    var_exp = new_model.likelihood.variational_expectations(f_mean, f_var, Y)\n",
    "    # compute variational expectation\n",
    "    var_exp = tf.reduce_sum(var_exp)\n",
    "    # get p(u_new|psi_old) and p(u_new,psi_new)\n",
    "    Kold = old_model.kernel(Znew)\n",
    "    Knew = new_model.kernel(Znew)\n",
    "    \n",
    "    q_mu = new_model.q_mu\n",
    "    q_sqrt = new_model.q_sqrt\n",
    "    \n",
    "    # compute KL divergences with posterior variational params of new GP\n",
    "    KL_q_pnew = get_KL(q_mu, q_sqrt, Knew)\n",
    "    KL_q_pold = get_KL(q_mu, q_sqrt, Kold)\n",
    "    KL_q_qold = get_KL(q_mu, q_sqrt, Kuu, mu_uu)\n",
    "    \n",
    "    # compute lower bound Lc\n",
    "    Lc = var_exp - KL_q_pnew + KL_q_pold - KL_q_qold\n",
    "    KL = [KL_q_pnew, KL_q_pold, KL_q_qold]\n",
    "    \n",
    "    return Lc, KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Kuu(model, Znew, Zold):\n",
    "    \n",
    "    K = model.kernel\n",
    "    \n",
    "    Kxx = K(Znew)\n",
    "    Kxm = K(Znew, Zold)\n",
    "    Kmm = K(Zold, Zold)\n",
    "    Kmminv = tf.linalg.inv(Kmm)\n",
    "    Kmx = tf.transpose(Kxm)\n",
    "    L_u = model.q_sqrt\n",
    "    A = tf.tensordot(L_u, tf.transpose(L_u),axes=1)[:,:,:,0]\n",
    "    B = Kmminv @ A @ Kmminv\n",
    "    \n",
    "    Kuu = (\n",
    "            Kxx - \n",
    "            (Kxm @ Kmminv @ Kmx) +\n",
    "            (Kxm @ B @ Kmx)\n",
    "            )\n",
    "    \n",
    "    mu_var = Kxm @ Kmminv @ model.q_mu\n",
    "    \n",
    "    return Kuu, mu_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_KL(q_mu, q_sqrt, K, mu=None):\n",
    "           \n",
    "    if mu == None:\n",
    "        pu = tfd.MultivariateNormalFullCovariance(covariance_matrix=K)\n",
    "    else:\n",
    "        pu = tfd.MultivariateNormalFullCovariance(mu[:,0], K)\n",
    "        \n",
    "    qu = tfd.MultivariateNormalTriL(q_mu[:,0],q_sqrt)    \n",
    "    \n",
    "    return tfd.kl_divergence(qu,pu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "max_X = 0.4\n",
    "X = np.sort(max_X*np.random.rand(N))[:, None]\n",
    "def true_f(X_input):\n",
    "    return (4.5 * np.cos(2 * np.pi * X_input + 1.5*np.pi) - \\\n",
    "           3 * np.sin(4.3 * np.pi * X_input + 0.3 * np.pi) + \\\n",
    "           5 * np.cos(7 * np.pi * X_input + 2.4 * np.pi))    \n",
    "\n",
    "Y = np.random.normal(true_f(X))\n",
    "\n",
    "plt.scatter(X,Y, alpha=0.5,marker=\"x\")\n",
    "Xt = np.linspace(0, 0.4, 1000)[:, None]\n",
    "Yt = true_f(Xt)\n",
    "_ = plt.plot(Xt, Yt, c=\"k\")\n",
    "plt.title(\"True f(X)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xold = X[0:int(len(X)/2)]\n",
    "Xnew = X[int(len(X)/2):]\n",
    "Yold = np.random.normal(true_f(Xold))\n",
    "Ynew = np.random.normal(true_f(Xnew))\n",
    "\n",
    "data_old = (Xold, Yold)\n",
    "data_new = (Xnew, Ynew)\n",
    "\n",
    "plt.scatter(Xold,Yold,alpha=0.5,marker=\"x\")\n",
    "plt.scatter(Xnew,Ynew, c=\"r\",alpha=0.5,marker=\"x\")\n",
    "Xt = np.linspace(0, 0.4, 1000)[:, None]\n",
    "Yt = true_f(Xt)\n",
    "_ = plt.plot(Xt, Yt, c=\"k\")\n",
    "plt.title(\"Data split into 2 batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise inducing points and SVGP for first half of data\n",
    "Zold = np.linspace(0,0.2,3)[:,None]\n",
    "m_old = gpflow.models.SVGP(kernel = gpflow.kernels.SquaredExponential(),\n",
    "                            likelihood = gpflow.likelihoods.Gaussian(), \n",
    "                            inducing_variable = Zold,\n",
    "                            num_data = len(Xold))\n",
    "gpflow.set_trainable(m_old.inducing_variable, False)\n",
    "m_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xold,Yold,alpha=0.5,marker=\"x\")\n",
    "Xt = np.linspace(0, max(Xold[:,0]), 100)[:, None]\n",
    "Yt, Yv = m_old.predict_y(Xt)\n",
    "_ = plt.plot(Xt, Yt, c=\"#ffa01f\")\n",
    "plt.fill_between(\n",
    "        Xt[:, 0],\n",
    "        (Yt - 2 * Yv ** 0.5)[:, 0],\n",
    "        (Yt + 2 * Yv ** 0.5)[:, 0],        \n",
    "        alpha=0.8,\n",
    "        lw=1.5,\n",
    "        color = \"#ffa01f\"\n",
    "    )\n",
    "plt.title(\"Predictions before training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logf = run_adam(m_old, 8000, data_old)\n",
    "plt.plot(np.arange(8000)[::10], logf)\n",
    "plt.xlabel(\"iteration\")\n",
    "_ = plt.ylabel(\"ELBO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xold,Yold,alpha=0.5,marker=\"x\")\n",
    "Xt = np.linspace(0, max(Xold[:,0]), 100)[:, None]\n",
    "Yt, Yv = m_old.predict_y(Xt)\n",
    "_ = plt.plot(Xt, Yt, c=\"#ffa01f\")\n",
    "plt.fill_between(\n",
    "        Xt[:, 0],\n",
    "        (Yt - 2 * Yv ** 0.5)[:, 0],\n",
    "        (Yt + 2 * Yv ** 0.5)[:, 0],        \n",
    "        alpha=0.8,\n",
    "        lw=1.5,\n",
    "        color = \"#ffa01f\"\n",
    "    )\n",
    "plt.title(\"Predictions after training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialise inducing points and SVGP for second half of data\n",
    "Znew = np.linspace(0,0.4,6)[:,None]\n",
    "\n",
    "m_new = gpflow.models.SVGP(kernel = gpflow.utilities.deepcopy(m_old.kernel),\n",
    "                            likelihood = gpflow.likelihoods.Gaussian(), \n",
    "                            inducing_variable = Znew)\n",
    "\n",
    "gpflow.set_trainable(m_new.inducing_variable, False)\n",
    "\n",
    "m_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xold,Yold,alpha=0.5,marker=\"x\")\n",
    "plt.scatter(Xnew,Ynew,alpha=0.5,marker=\"x\", color=\"r\")\n",
    "Xt = np.linspace(0, max(Xnew[:,0]), 100)[:, None]\n",
    "Yt, Yv = m_new.predict_y(Xt)\n",
    "_ = plt.plot(Xt, Yt, c=\"#ffa01f\")\n",
    "plt.fill_between(\n",
    "        Xt[:, 0],\n",
    "        (Yt - 2 * Yv ** 0.5)[:, 0],\n",
    "        (Yt + 2 * Yv ** 0.5)[:, 0],        \n",
    "        alpha=0.8,\n",
    "        lw=1.5,\n",
    "        color = \"#ffa01f\"\n",
    "    )\n",
    "plt.title(\"New Data, predictions before training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step, Lc, KL = run_continuous_opt(m_old,m_new,data_new,Znew,Zold,maxiter=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(step,Lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(step,np.reshape(KL, (len(KL),3)))\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xold,Yold,alpha=0.5,marker=\"x\")\n",
    "plt.scatter(Xnew,Ynew,alpha=0.5,marker=\"x\", color=\"r\")\n",
    "Xt = np.linspace(0, max(Xnew[:,0]), 100)[:, None]\n",
    "Yt, Yv = m_new.predict_y(Xt)\n",
    "_ = plt.plot(Xt, Yt, c=\"#ffa01f\")\n",
    "plt.fill_between(\n",
    "        Xt[:, 0],\n",
    "        (Yt - 2 * Yv ** 0.5)[:, 0],\n",
    "        (Yt + 2 * Yv ** 0.5)[:, 0],        \n",
    "        alpha=0.8,\n",
    "        lw=1.5,\n",
    "        color = \"#ffa01f\"\n",
    "    )\n",
    "plt.title(\"New Data, predictions after training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
